{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 1., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1.]),\n",
       " array([1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard rnn\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # number of hidden units\n",
    "vocab_size = 5000 # number of unique words in training data\n",
    "\n",
    "# initialize weights\n",
    "np.random.seed(42)\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
    "W_hx = np.random.randn(hidden_size, vocab_size) * 0.01  # Input to hidden\n",
    "W_s = np.random.randn(vocab_size, hidden_size) * 0.01  # Hidden to output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1))  # Bias for hidden layer\n",
    "b_s = np.zeros((vocab_size, 1))  # Bias for output layer\n",
    "\n",
    "corpus = \"Napoleon was the Emperor of France\"\n",
    "\n",
    "# tokenization - split corpus into words\n",
    "tokens = corpus.lower().split()\n",
    "\n",
    "# create vocab\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# word to index\n",
    "word_to_index = {w: idx for idx, w in enumerate(vocab)}\n",
    "index_to_word = {idx: w for idx, w in enumerate(vocab)}\n",
    "\n",
    "# word -> one-hot vectors\n",
    "def word_to_one_hot(word):\n",
    "    one_hot_vector = np.zeros(vocab_size)\n",
    "    one_hot_vector[word_to_index[word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def sentence_to_one_hot_vectors(sentence):\n",
    "    # sentence into tokens\n",
    "    tokens = sentence.lower().split()\n",
    "    # convert each word in sentence to a one-hot vector\n",
    "    one_hot_vectors = [ word_to_one_hot(word) for word in tokens]\n",
    "    return one_hot_vectors\n",
    "\n",
    "one_hot_vectors = sentence_to_one_hot_vectors(corpus)\n",
    "one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Cross-entropy loss: 1.4170300162778335\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probs = softmax(logits)\n",
    "# print(probs)\n",
    "\n",
    "\n",
    "def forward_pass(inputs, h_prev, W_hh, W_hx, W_s, b_h, b_s):\n",
    "    \"\"\"\n",
    "    inputs: List of one-hot encoded words (as numpy arrays)\n",
    "    h_prev: Previous hidden state\n",
    "    W_hh: Hidden to hidden weight matrix\n",
    "    W_hx: Input to hidden weight matrix\n",
    "    W_s: Hidden to output weight matrix\n",
    "    b_h: Hidden layer bias\n",
    "    b_s: Output layer bias\n",
    "    \"\"\"\n",
    "    outputs, h_states = [], []\n",
    "\n",
    "    # empty list to store the outputs and hidden states at each time step\n",
    "    for x in inputs:\n",
    "        # calc new hidden state for current time step\n",
    "        print(W_hx.shape, x.shape, W_hh.shape, h_prev.shape)\n",
    "        h = np.tanh(np.dot(W_hx, x) + np.dot(W_hh, h_prev) + b_h)\n",
    "\n",
    "        # calc unormalized log prob for next words\n",
    "        y = np.dot(W_s, h) + b_s\n",
    "\n",
    "        # apply softmax to get probs\n",
    "        p = softmax(y)\n",
    "\n",
    "        outputs.append(p)\n",
    "        h_states.append(h)\n",
    "\n",
    "        # Update hidden state to current state\n",
    "        h_prev = h\n",
    "    return outputs, h_states, h_prev\n",
    "# Example parameters\n",
    "vocab_size, hidden_size = 3, 2\n",
    "W_hh = np.random.randn(hidden_size, hidden_size)\n",
    "W_hx = np.random.randn(hidden_size, vocab_size)\n",
    "W_s = np.random.randn(vocab_size, hidden_size)\n",
    "b_h = np.zeros((hidden_size, 1))\n",
    "b_s = np.zeros((vocab_size, 1))\n",
    "\n",
    "# Initial hidden state\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Example one-hot encoded input for the word \"cat\" in a 3-word vocabulary\n",
    "x = np.array([[1], [0], [0]])\n",
    "\n",
    "# Forward pass\n",
    "# y_logits, h_next = forward_pass(x, h_prev, W_hh, W_hx, W_s, b_h, b_s)\n",
    "# print(\"Output logits:\", y_logits)\n",
    "\n",
    "def predict_output(probabilities):\n",
    "    # predicted word index with the one with max probability\n",
    "    return np.argmax(probabilities, axis = 0)\n",
    "print(predict_output(probs))\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: The one-hot encoded vector of the true next word\n",
    "    y_pred: The predicted probability distribution for the next word\n",
    "    \"\"\"\n",
    "    # Multiply the true distribution with the log of predicted, sum it up, and negate the value\n",
    "    loss = -np.sum(y_true * np.log(y_pred))\n",
    "    return loss\n",
    "\n",
    "y_true = np.array([0, 1, 0])  \n",
    "loss = cross_entropy_loss(y_true, probs)\n",
    "print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m true_word, output_prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mtrue_words\u001b[49m, outputs):\n\u001b[1;32m      5\u001b[0m     pred_word_index \u001b[38;5;241m=\u001b[39m predict_output(output_prob)\n\u001b[1;32m      6\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(pred_word_index)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_words' is not defined"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "loss = 0\n",
    "\n",
    "for true_word, output_prob in zip(true_words, outputs):\n",
    "    pred_word_index = predict_output(output_prob)\n",
    "    predictions.append(pred_word_index)\n",
    "\n",
    "    # Calculate and accumulate the loss\n",
    "    loss += cross_entropy_loss(true_word, output_prob)\n",
    "\n",
    "# To get average loss per time step\n",
    "average_loss = loss / len(true_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_char_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', ' ', 'r', 'e', 'h', 't', 'l']\n",
      "data has 11 characters, 7 unique.\n",
      "{'o': 0, ' ': 1, 'r': 2, 'e': 3, 'h': 4, 't': 5, 'l': 6}\n",
      "{0: 'o', 1: ' ', 2: 'r', 3: 'e', 4: 'h', 5: 't', 6: 'l'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize weight matrices U, V, W from random distribution and bias b, c with zeros\n",
    "\n",
    "# data loading\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars) # number of unique charactors\n",
    "print(\"data has %d characters, %d unique.\" % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "# print(char_to_ix)\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "# print(ix_to_char)\n",
    "\n",
    "# hyperparams\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model params\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input -> hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden -> hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden -> output\n",
    "\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "# training\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
