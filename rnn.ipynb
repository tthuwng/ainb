{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 1., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1.]),\n",
       " array([1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard rnn\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # number of hidden units\n",
    "vocab_size = 5000 # number of unique words in training data\n",
    "\n",
    "# initialize weights\n",
    "np.random.seed(42)\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
    "W_hx = np.random.randn(hidden_size, vocab_size) * 0.01  # Input to hidden\n",
    "W_s = np.random.randn(vocab_size, hidden_size) * 0.01  # Hidden to output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1))  # Bias for hidden layer\n",
    "b_s = np.zeros((vocab_size, 1))  # Bias for output layer\n",
    "\n",
    "corpus = \"Napoleon was the Emperor of France\"\n",
    "\n",
    "# tokenization - split corpus into words\n",
    "tokens = corpus.lower().split()\n",
    "\n",
    "# create vocab\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# word to index\n",
    "word_to_index = {w: idx for idx, w in enumerate(vocab)}\n",
    "index_to_word = {idx: w for idx, w in enumerate(vocab)}\n",
    "\n",
    "# word -> one-hot vectors\n",
    "def word_to_one_hot(word):\n",
    "    one_hot_vector = np.zeros(vocab_size)\n",
    "    one_hot_vector[word_to_index[word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def sentence_to_one_hot_vectors(sentence):\n",
    "    # sentence into tokens\n",
    "    tokens = sentence.lower().split()\n",
    "    # convert each word in sentence to a one-hot vector\n",
    "    one_hot_vectors = [ word_to_one_hot(word) for word in tokens]\n",
    "    return one_hot_vectors\n",
    "\n",
    "one_hot_vectors = sentence_to_one_hot_vectors(corpus)\n",
    "one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits: [[-0.27834095]\n",
      " [ 0.02297051]]\n",
      "next hidden state: [[-0.63506421]\n",
      " [-0.30803362]\n",
      " [ 0.34092135]]\n",
      "0\n",
      "Cross-entropy loss: 1.4170300162778335\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probs = softmax(logits)\n",
    "# print(probs)\n",
    "\n",
    "\n",
    "def forward_pass(x, h_prev, W_hh, W_hx, W_s, b_h, b_s):\n",
    "    h_next = np.tanh(np.dot(W_hx, x) + np.dot(W_hh, h_prev) + b_h)\n",
    "    y = np.dot(W_s, h_next) + b_s\n",
    "    return y, h_next\n",
    "# Example parameters\n",
    "vocab_size, hidden_size = 3, 2\n",
    "W_hh = np.random.randn(hidden_size, hidden_size)\n",
    "W_hx = np.random.randn(hidden_size, vocab_size)\n",
    "W_s = np.random.randn(vocab_size, hidden_size)\n",
    "b_h = np.zeros((hidden_size, 1))\n",
    "b_s = np.zeros((vocab_size, 1))\n",
    "\n",
    "# Initial hidden state\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Example one-hot encoded input for the word \"cat\" in a 3-word vocabulary\n",
    "x = np.array([[1], [0], [0]])\n",
    "\n",
    "# Forward pass\n",
    "y_logits, h_next = forward_pass(x, h_prev, W_hh, W_hx, W_s, b_h, b_s)\n",
    "print(\"Output logits:\", h_next)\n",
    "print(\"next hidden state:\", y_logits)\n",
    "\n",
    "\n",
    "def predict_output(probabilities):\n",
    "    # predicted word index with the one with max probability\n",
    "    return np.argmax(probabilities, axis = 0)\n",
    "print(predict_output(probs))\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: The one-hot encoded vector of the true next word\n",
    "    y_pred: The predicted probability distribution for the next word\n",
    "    \"\"\"\n",
    "    # Multiply the true distribution with the log of predicted, sum it up, and negate the value\n",
    "    loss = -np.sum(y_true * np.log(y_pred))\n",
    "    return loss\n",
    "\n",
    "y_true = np.array([0, 1, 0])  \n",
    "loss = cross_entropy_loss(y_true, probs)\n",
    "print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.27742029 0.42644415 0.29613556]\n",
      "Predicted word index: 1\n",
      "Cross-entropy loss: 0.8522738727746588\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dimensions\n",
    "vocab_size = 3  # Vocabulary size\n",
    "hidden_size = 2  # Hidden layer size\n",
    "\n",
    "# Initialize weights and biases with example values\n",
    "W_hx = np.array([[0.5, -0.2, 0.1], [0.3, 0.4, -0.5]])  # (2, 3)\n",
    "W_hh = np.array([[0.1, 0.4], [0.3, 0.7]])  # (2, 2)\n",
    "W_s = np.array([[0.2, -0.3], [0.5, 0.7], [-0.1, 0.4]])  # (3, 2)\n",
    "b_h = np.array([[0.0], [0.0]])  # (2, 1)\n",
    "b_s = np.array([[0.0], [0.0], [0.0]])  # (3, 1)\n",
    "h_prev = np.array([[0.0], [0.0]])  # (2, 1)\n",
    "\n",
    "# Example one-hot encoded input and true next word\n",
    "x = np.array([[1], [0], [0]])  # (3, 1) Input for \"word1\"\n",
    "y_true = np.array([[0], [1], [0]])  # (3, 1) True next word is \"word2\"\n",
    "\n",
    "# Forward pass\n",
    "h_next = np.tanh(np.dot(W_hx, x) + np.dot(W_hh, h_prev) + b_h)\n",
    "y_logits = np.dot(W_s, h_next) + b_s\n",
    "probabilities = softmax(y_logits)\n",
    "\n",
    "# Prediction\n",
    "predicted_index = np.argmax(probabilities)\n",
    "\n",
    "# Cross-entropy loss\n",
    "loss = -np.sum(y_true * np.log(probabilities))\n",
    "\n",
    "print(\"Probabilities:\", probabilities.ravel())\n",
    "print(\"Predicted word index:\", predicted_index)\n",
    "print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_char_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', ' ', 'r', 'e', 'h', 't', 'l']\n",
      "data has 11 characters, 7 unique.\n",
      "{'o': 0, ' ': 1, 'r': 2, 'e': 3, 'h': 4, 't': 5, 'l': 6}\n",
      "{0: 'o', 1: ' ', 2: 'r', 3: 'e', 4: 'h', 5: 't', 6: 'l'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize weight matrices U, V, W from random distribution and bias b, c with zeros\n",
    "\n",
    "# data loading\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars) # number of unique charactors\n",
    "print(\"data has %d characters, %d unique.\" % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "# print(char_to_ix)\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "# print(ix_to_char)\n",
    "\n",
    "# hyperparams\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model params\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input -> hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden -> hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden -> output\n",
    "\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "# training\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
